# Module 1 - Explore compute and storage options for data engineering workloads

This module teaches ways to structure the data lake, and to optimize the files for exploration, streaming, and batch workloads. The student will learn how to organize the data lake into levels of data refinement as they transform files through batch and stream processing. Then they will learn how to create indexes on their datasets, such as CSV, JSON, and Parquet files, and use them for potential query and workload acceleration.

In this module, the student will be able to:

- Combine streaming and batch processing with a single pipeline
- Organize the data lake into levels of file transformation
- Index data lake storage for query and workload acceleration

## Lab details

- [Module 1 - Explore compute and storage options for data engineering workloads](#module-1---explore-compute-and-storage-options-for-data-engineering-workloads)
  - [Lab details](#lab-details)
  - [Lab 1 - Delta Lake architecture](#lab-1---delta-lake-architecture)
    - [Before the hands-on lab](#before-the-hands-on-lab)
      - [Task 1: Create and configure the Azure Databricks workspace](#task-1-create-and-configure-the-azure-databricks-workspace)
    - [Exercise 1: Complete the lab notebook](#exercise-1-complete-the-lab-notebook)
      - [Task 1: Clone the Databricks archive](#task-1-clone-the-databricks-archive)
      - [Task 2: Complete the following notebook](#task-2-complete-the-following-notebook)
  - [Lab 2 - Working with Apache Spark in Synapse Analytics](#lab-2---working-with-apache-spark-in-synapse-analytics)
    - [Before the hands-on lab](#before-the-hands-on-lab-1)
      - [Task 1: Create and configure the Azure Synapse Analytics workspace](#task-1-create-and-configure-the-azure-synapse-analytics-workspace)
      - [Task 2: Create and configure additional resources for this lab](#task-2-create-and-configure-additional-resources-for-this-lab)
    - [Exercise 1: Load and data with Spark](#exercise-1-load-and-data-with-spark)
      - [Task 1: Index the Data Lake storage with Hyperspace](#task-1-index-the-data-lake-storage-with-hyperspace)
      - [Task 2: Explore the Data Lake storage with the MSSparkUtil library](#task-2-explore-the-data-lake-storage-with-the-mssparkutil-library)
    - [Resources](#resources)

## Lab 1 - Delta Lake architecture

In this lab, you will use an Azure Databricks workspace and perform Structured Streaming with batch jobs by using Delta Lake. You need to complete the exercises within a Databricks Notebook. To begin, you need to have access to an Azure Databricks workspace. If you do not have a workspace available, follow the instructions below. Otherwise, you can skip to the bottom of the page to [Clone the Databricks archive](#clone-the-databricks-archive).

### Before the hands-on lab

> **Note:** Only complete the `Before the hands-on lab` steps if you are **not** using a hosted lab environment, and are instead using your own Azure subscription. Otherwise, skip ahead to Exercise 1.

Before stepping through the exercises in this lab, make sure you have access to an Azure Databricks workspace with an available cluster. Perform the tasks below to configure the workspace.

#### Task 1: Create and configure the Azure Databricks workspace

**If you are not using a hosted lab environment**, follow the [lab 01 setup instructions](https://github.com/solliancenet/microsoft-data-engineering-ilt-deploy/blob/main/setup/01/lab-01-setup.md) to manually create and configure the workspace.

### Exercise 1: Complete the lab notebook

#### Task 1: Clone the Databricks archive

1. If you do not currently have your Azure Databricks workspace open: in the Azure portal, navigate to your deployed Azure Databricks workspace and select **Launch Workspace**.
1. In the left pane, select **Workspace** > **Users**, and select your username (the entry with the house icon).
1. In the pane that appears, select the arrow next to your name, and select **Import**.

  ![The menu option to import the archive](media/import-archive.png)

1. In the **Import Notebooks** dialog box, select the URL and paste in the following URL:

 ```
  https://github.com/solliancenet/microsoft-learning-paths-databricks-notebooks/blob/master/data-engineering/DBC/11-Delta-Lake-Architecture.dbc?raw=true
  Here are Creds:
  <inject key="AzureAdUserEmail"></inject>
  <inject key="AzureAdUserPassword"></inject>
  
 ```

1. Select **Import**.
1. Select the **11-Delta-Lake-Architecture** folder that appears.

#### Task 2: Complete the following notebook

Open the **1-Delta-Architecture** notebook. Make sure you attach your cluster to the notebook before following the instructions and running the cells within.

Within the notebook, you will explore combining streaming and batch processing with a single pipeline.

> After you've completed the notebook, return to this screen, and continue to the next lab.

## Lab 2 - Working with Apache Spark in Synapse Analytics

This lab demonstrates the experience of working with Apache Spark in Azure Synapse Analytics. You will learn how to connect an Azure Synapse Analytics workspace to an Azure Data Explorer workspace using a Linked Service and then load data from one of its databases using a Spark notebook. You will also learn how to use libraries like Hyperspace and MSSparkUtil to optimize the experience of working with Data Lake storage accounts from Spark notebooks. In addition to Data Explorer and Data Lake storage, the data enrichment process will also use historical data from a SQL Pool. In the end, you will learn how to publish the enriched data back into the Data Lake and consume it with the SQL Built-in Pool and Power BI.

After completing the lab, you will understand the main steps of an end-to-end data enrichment process that uses Spark in an Azure Synapse Analytics workspace.

### Before the hands-on lab

> **Note:** Only complete the `Before the hands-on lab` steps if you are **not** using a hosted lab environment, and are instead using your own Azure subscription. Otherwise, skip ahead to Exercise 1.

Before stepping through the exercises in this lab, make sure you have properly configured your Azure Synapse Analytics workspace. Perform the tasks below to configure the workspace.

#### Task 1: Create and configure the Azure Synapse Analytics workspace

>**NOTE**
>
>If you have already created and configured the Synapse Analytics workspace while running one of the other labs available in this repo, you must not perform this task again and you can move on to the next task. The labs are designed to share the Synapse Analytics workspace, so you only need to create it once.

**If you are not using a hosted lab environment**, follow the instructions in [Deploy your Azure Synapse Analytics workspace](https://github.com/solliancenet/microsoft-data-engineering-ilt-deploy/blob/main/setup/01/asa-workspace-deploy.md) to create and configure the workspace.

#### Task 2: Create and configure additional resources for this lab

**If you are not using a hosted lab environment**, follow the instructions in [Deploy resources for Lab 02](https://github.com/solliancenet/microsoft-data-engineering-ilt-deploy/blob/main/setup/01/lab-02-deploy.md) to deploy additional resources for this lab. Once deployment is complete, you are ready to proceed with the exercises in this lab.

QA Test 
```
  https://github.com/solliancenet/microsoft-learning-paths-databricks-notebooks/blob/master/data-engineering/DBC/11-Delta-Lake-Architecture.dbc?raw=true
  Test QA Creds:
  <inject key="AzureAdUserEmail"></inject>
  <inject key="AzureAdUserPassword"></inject>
Deployment ID : <inject key="Deployment ID"></inject>
Labvm Admin Username : <inject key="Labvm Admin Username"></inject>
Labvm Admin Password : <inject key="Labvm Admin Password"></inject>
Labvm DNS Name : <inject key="Labvm DNS Name"></inject>
  
 ```


```
Test Command with colors
Test QA Creds:
AzureAdUserEmail : <inject key="AzureAdUserEmail"></inject>
AzureAdUserPassword : <inject key="AzureAdUserPassword"></inject>
Deployment ID : <inject key="Deployment ID"></inject>
Labvm Admin Username : <inject key="Labvm Admin Username"></inject>
Labvm Admin Password : <inject key="Labvm Admin Password"></inject>
Labvm DNS Name : <inject key="Labvm DNS Name" style="color:blue"></inject>
```

```
Test Command color above line
Test QA Creds:
Labvm DNS Name : <inject key="Labvm DNS Name" style="color:yellow"></inject>
AzureAdUserEmail : <inject key="AzureAdUserEmail"></inject>
AzureAdUserPassword : <inject key="AzureAdUserPassword"></inject>
Deployment ID : <inject key="Deployment ID"></inject>
Labvm Admin Username : <inject key="Labvm Admin Username"></inject>
Labvm Admin Password : <inject key="Labvm Admin Password"></inject>
```

test copy icon

> AzureAdUserEmail :** <inject key="AzureAdUserEmail" enableCopy="false"/>**
> AzureAdUserPassword :** <inject key="AzureAdUserPassword" enableCopy="false"/>**
> Deployment ID :** <inject key="Deployment ID" enableCopy="false"/>**
